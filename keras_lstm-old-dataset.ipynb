{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras_timeseries.py\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten    \n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from keras import backend as K\n",
    "\n",
    "#df = pd.read_csv(\"customers_timeseries_updated.csv\")\n",
    "df = pd.read_csv(\"100_timeseries_checking_to_share.csv\")\n",
    "\n",
    "#use column names\n",
    "df = df[['client_debtor_number','dates','fv_cost']]\n",
    "\n",
    "# number of customers\n",
    "len(df['client_debtor_number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different customers , 1015130-mostly zero with one dip\n",
    "# 1015193 - more debt after some time,\n",
    "\n",
    "df_1015130 = df[df['client_debtor_number'] == 1015193]\n",
    "df_1015130.head()\n",
    "del df_1015130['client_debtor_number']\n",
    "# to get client number from list of client numbers use; \n",
    "#df[df['client_number'].isin(list_of_values)]\\n\",\n",
    "#frequency counts for each customer\n",
    "# df['client_number'].value_counts()\n",
    "\n",
    "#df_1015130['client_debtor_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3192    01/01/2010\n",
      "3193    02/01/2010\n",
      "3194    03/01/2010\n",
      "3195    04/01/2010\n",
      "3196    05/01/2010\n",
      "3197    06/01/2010\n",
      "3198    07/01/2010\n",
      "3199    08/01/2010\n",
      "3200    09/01/2010\n",
      "3201    10/01/2010\n",
      "3202    11/01/2010\n",
      "3203    12/01/2010\n",
      "3204    13/01/2010\n",
      "3205    14/01/2010\n",
      "3206    15/01/2010\n",
      "3207    16/01/2010\n",
      "3208    17/01/2010\n",
      "3209    18/01/2010\n",
      "3210    19/01/2010\n",
      "3211    20/01/2010\n",
      "3212    21/01/2010\n",
      "3213    22/01/2010\n",
      "3214    23/01/2010\n",
      "3215    24/01/2010\n",
      "3216    25/01/2010\n",
      "3217    26/01/2010\n",
      "3218    27/01/2010\n",
      "3219    28/01/2010\n",
      "3220    29/01/2010\n",
      "3221    30/01/2010\n",
      "           ...    \n",
      "6640    29/08/2018\n",
      "6641    30/08/2018\n",
      "6642    31/08/2018\n",
      "6643    01/09/2018\n",
      "6644    02/09/2018\n",
      "6645    03/09/2018\n",
      "6646    04/09/2018\n",
      "6647    05/09/2018\n",
      "6648    06/09/2018\n",
      "6649    07/09/2018\n",
      "6650    08/09/2018\n",
      "6651    09/09/2018\n",
      "6652    10/09/2018\n",
      "6653    11/09/2018\n",
      "6654    12/09/2018\n",
      "6655    13/09/2018\n",
      "6656    14/09/2018\n",
      "6657    15/09/2018\n",
      "6658    16/09/2018\n",
      "6659    17/09/2018\n",
      "6660    18/09/2018\n",
      "6661    19/09/2018\n",
      "6662    20/09/2018\n",
      "6663    21/09/2018\n",
      "6664    22/09/2018\n",
      "6665    23/09/2018\n",
      "6666    24/09/2018\n",
      "6667    25/09/2018\n",
      "6668    26/09/2018\n",
      "6669    27/09/2018\n",
      "Name: dates, Length: 3478, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>fv_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3192</th>\n",
       "      <td>01/01/2010</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>02/01/2010</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>03/01/2010</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>04/01/2010</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>05/01/2010</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dates  fv_cost\n",
       "3192  01/01/2010      0.0\n",
       "3193  02/01/2010      0.0\n",
       "3194  03/01/2010      0.0\n",
       "3195  04/01/2010      0.0\n",
       "3196  05/01/2010      0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "#import seaborn; seaborn.set()\n",
    "#df_1015130.plot()\n",
    "\n",
    "print(df_1015130['dates'])\n",
    "df_1015130.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data  3192       0.000000\n",
      "3193       0.000000\n",
      "3194       0.000000\n",
      "3195       0.000000\n",
      "3196       0.000000\n",
      "3197       0.000000\n",
      "3198       0.000000\n",
      "3199       0.000000\n",
      "3200       0.000000\n",
      "3201       0.000000\n",
      "3202       0.000000\n",
      "3203       0.000000\n",
      "3204       0.000000\n",
      "3205       0.000000\n",
      "3206       0.000000\n",
      "3207       0.000000\n",
      "3208       0.000000\n",
      "3209       0.000000\n",
      "3210       0.000000\n",
      "3211       0.000000\n",
      "3212       0.000000\n",
      "3213       0.000000\n",
      "3214       0.000000\n",
      "3215       0.000000\n",
      "3216       0.000000\n",
      "3217       0.000000\n",
      "3218       0.000000\n",
      "3219       0.000000\n",
      "3220       0.000000\n",
      "3221       0.000000\n",
      "           ...     \n",
      "6354   -5690.307208\n",
      "6355   -5693.596074\n",
      "6356   -5693.596074\n",
      "6357   -5693.596074\n",
      "6358   -5802.687345\n",
      "6359   -5806.041864\n",
      "6360   -5809.396831\n",
      "6361   -5812.752246\n",
      "6362   -6011.859327\n",
      "6363   -6015.335480\n",
      "6364   -6018.812098\n",
      "6365   -5768.143897\n",
      "6366   -5768.143897\n",
      "6367   -5768.143897\n",
      "6368   -5853.265493\n",
      "6369   -5856.640747\n",
      "6370   -5860.016452\n",
      "6371   -5863.392609\n",
      "6372   -5903.222380\n",
      "6373   -5906.621757\n",
      "6374   -5910.021587\n",
      "6375   -5928.551726\n",
      "6376   -5931.961728\n",
      "6377   -5935.372186\n",
      "6378   -5938.783100\n",
      "6379   -5942.194471\n",
      "6380   -5945.606297\n",
      "6381   -5949.018579\n",
      "6382   -5952.431317\n",
      "6383   -5955.844512\n",
      "Name: fv_cost, Length: 3192, dtype: float64\n",
      "test_data  6384   -5959.258162\n",
      "6385   -6737.111997\n",
      "6386   -6754.003443\n",
      "6387   -6757.900606\n",
      "6388   -6761.798290\n",
      "6389   -6765.696494\n",
      "6390   -6769.595220\n",
      "6391   -6773.494467\n",
      "6392   -6777.394236\n",
      "6393   -6777.394236\n",
      "6394   -7121.017393\n",
      "6395   -7276.424722\n",
      "6396   -7280.626690\n",
      "6397   -7284.829219\n",
      "6398   -7289.032311\n",
      "6399   -7293.235964\n",
      "6400   -7297.440179\n",
      "6401   -7301.644957\n",
      "6402   -7468.044161\n",
      "6403   -7163.503022\n",
      "6404   -7167.623207\n",
      "6405   -7171.743943\n",
      "6406   -7175.865230\n",
      "6407   -7212.493977\n",
      "6408   -7216.636267\n",
      "6409   -7220.779110\n",
      "6410   -7224.922508\n",
      "6411   -7229.066459\n",
      "6412   -7233.210964\n",
      "6413   -7237.356023\n",
      "           ...     \n",
      "6640   -5274.093864\n",
      "6641   -5277.081687\n",
      "6642   -5280.069909\n",
      "6643   -5465.326532\n",
      "6644   -5468.427140\n",
      "6645   -6245.967889\n",
      "6646   -6249.543445\n",
      "6647   -6221.837195\n",
      "6648   -6225.395045\n",
      "6649   -6248.457516\n",
      "6650   -6252.028257\n",
      "6651   -6255.599476\n",
      "6652   -6259.171172\n",
      "6653   -6282.247492\n",
      "6654   -6292.333465\n",
      "6655   -6356.441931\n",
      "6656   -6360.068513\n",
      "6657   -6363.695579\n",
      "6658   -6367.323130\n",
      "6659   -5519.399170\n",
      "6660   -5439.808383\n",
      "6661   -5442.877989\n",
      "6662   -5445.948006\n",
      "6663   -5449.018433\n",
      "6664   -5452.089270\n",
      "6665   -5545.939639\n",
      "6666   -5549.066874\n",
      "6667   -5630.217661\n",
      "6668   -5633.393498\n",
      "6669   -5600.030996\n",
      "Name: fv_cost, Length: 286, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#df[df.Date.str.match(r'^2018-06')]\n",
    "#df[df[\\\"col\\\"].str.contains('this|that')==False]\n",
    "# splitting train and test datasets \n",
    "#training 2010-2017, testing on 2018 data\\\n",
    "train_x = df_1015130[df_1015130['dates'].str.contains('/2018') == False]\n",
    "test_x = df_1015130[df_1015130['dates'].str.contains('/2018') == True]\n",
    "#print(test_x.dates)\n",
    "train_x_cust = train_x['fv_cost']\n",
    "test_x_cust = test_x['fv_cost']\n",
    "\n",
    "print(\"train_data \",train_x_cust)\n",
    "print(\"test_data \", test_x_cust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://drivendata.co/blog/benchmark-cold-start-lstm-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     3192.000000\n",
       "mean     -3438.513077\n",
       "std       4674.396262\n",
       "min     -24863.169200\n",
       "25%      -5287.090673\n",
       "50%      -1821.800782\n",
       "75%          0.000000\n",
       "max          0.000000\n",
       "Name: fv_cost, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x['fv_cost'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print (train_x[train_x['fv_cost'].str.contains(2016)])\n",
    "n_input = 2016\n",
    "\n",
    "train_x[(train_x == n_input).any(1)].stack()[lambda x: x != n_input].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3189, 1, 3)\n",
      "(3189,)\n",
      "MinMaxScaler(copy=True, feature_range=(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lag_feature(df, lag=1):\n",
    "    if not type(df) == pd.DataFrame:\n",
    "        df = pd.DataFrame(df, columns=['fv_cost'])\n",
    "    \n",
    "    def rename_lag(ser, j):\n",
    "        ser.name = ser.name + f'_{j}'\n",
    "        return ser\n",
    "        \n",
    "    # add a column lagged by `i` steps\n",
    "    for i in range(1, lag + 1):\n",
    "        df = df.join(df.fv_cost.shift(i).pipe(rename_lag, i))\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#lagged_data=pd.DataFrame(lag_feature(train_x_cust))\n",
    "\n",
    "# X, y format taking the first column (original time series) to be the y\n",
    "#X = lagged_data.drop('fv_cost', axis=1).values\n",
    "#y = lagged_data.fv_cost.values\n",
    "\n",
    "#train_x_cust = lagged_data[:,0]\n",
    "#train_y_cust = lagged_data[:,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_training_data(series_data, lag):\n",
    "    \" Converts a series of data into a lagged, scaled sample.\"\n",
    "    # scale training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    cost_vals = scaler.fit_transform(series_data.values.reshape(-1, 1))\n",
    "    \n",
    "    # convert series to lagged features\n",
    "    cost_lagged = lag_feature(cost_vals, lag=lag)\n",
    "\n",
    "    # X, y format taking the first column (original time series) to be the y\n",
    "    X = cost_lagged.drop('fv_cost', axis=1).values\n",
    "    y = cost_lagged.fv_cost.values\n",
    "    \n",
    "    # keras expects 3 dimensional X\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "train_x_cust,train_y_cust, scaler = prepare_training_data(train_x['fv_cost'], 3)\n",
    "test_x_cust,test_y_cust,scalar = prepare_training_data(test_x['fv_cost'], 3)\n",
    "print(train_x_cust.shape)\n",
    "print(train_y_cust.shape)\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "# lag of 24 to simulate smallest cold start window. Our series\n",
    "# will be converted to a num_timesteps x lag size matrix\n",
    "lag =  3\n",
    "\n",
    "# model parameters\n",
    "num_neurons = 24\n",
    "batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "batch_input_shape=(batch_size, 1, lag)\n",
    "\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "#add convolution layer\n",
    "\n",
    "#Convolution2D (https://keras.io/layers/convolutional/) expects the input to be in the format (samples, rows, cols, channels), \n",
    "#which is \"channels-last\". data is in the format (samples, channels, rows, cols). You should be able to fix\n",
    "#this using the optional keyword data_format = 'channels_first' when declaring the Convolution2D layer else use\n",
    "# input_shape=(3,1).\n",
    "model.add(Conv1D(filters=24,batch_size=1, kernel_size=3, strides=3, padding=\"same\",activation='relu',dilation_rate=1, input_shape=(1, 3),data_format='channels_first'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Flatten()) # as flatten is converting data into 1D but we need 3D for our 3lags data here\n",
    "\n",
    "# add LSTM layer - stateful MUST be true here in \n",
    "# order to learn the patterns within a series\n",
    "model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "\n",
    "# followed by a dense layer with a single output for regression\n",
    "model.add(Dense(1))\n",
    "# we can add dropoutlayer after dense as well again\n",
    "# compile\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "num_training_series = consumption_train.series_id.nunique()\n",
    "num_passes_through_data = 3\n",
    "\n",
    "for i in tqdm(range(num_passes_through_data), \n",
    "              total=num_passes_through_data, \n",
    "              desc='Learning Consumption Trends - Epoch'):\n",
    "    \n",
    "    # reset the LSTM state for training on each series\n",
    "    for ser_id, ser_data in consumption_train.groupby('series_id'):\n",
    "\n",
    "        # prepare the data\n",
    "        X, y, scaler = prepare_training_data(ser_data.consumption, lag)\n",
    "\n",
    "        # fit the model: note that we don't shuffle batches (it would ruin the sequence)\n",
    "        # and that we reset states only after an entire X has been fit, instead of after\n",
    "        # each (size 1) batch, as is the case when stateful=False\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 18s - loss: 0.0847\n",
      "Epoch 2/10\n",
      " - 19s - loss: 0.0563\n",
      "Epoch 3/10\n",
      " - 19s - loss: 0.0532\n",
      "Epoch 4/10\n",
      " - 20s - loss: 0.0519\n",
      "Epoch 5/10\n",
      " - 19s - loss: 0.0485\n",
      "Epoch 6/10\n",
      " - 18s - loss: 0.0443\n",
      "Epoch 7/10\n",
      " - 18s - loss: 0.0403\n",
      "Epoch 8/10\n",
      " - 17s - loss: 0.0348\n",
      "Epoch 9/10\n",
      " - 18s - loss: 0.0326\n",
      "Epoch 10/10\n",
      " - 18s - loss: 0.0304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2c5ed400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "model.fit(train_x_cust, train_y_cust, epochs=10, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "# plot metrics\n",
    "#pyplot.plot(history.history['mse'])\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.26 MSE (0.51 RMSE)\n",
      "Test Score: 0.24 MSE (0.49 RMSE)\n",
      "unscaled test score -9397.540099 MSE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-9188.85858328]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Estimate model performance\n",
    "# default batch size is 32 so we need to give batch size explicitly if we want different batch size or \n",
    "# input size should be divisible by batch size for stateful LSTM\n",
    "trainScore = model.evaluate(train_x_cust, train_y_cust, batch_size=1, verbose=2)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(test_x_cust, test_y_cust, batch_size=1, verbose=2)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))\n",
    "\n",
    "print('unscaled test score %f MSE' %(scaler.inverse_transform(testScore)))\n",
    "#print (math.sqrt(scaler.inverse_transform(testScore)))\n",
    "scaler.inverse_transform(trainScore)\n",
    "#scaler.inverse_transform(testScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2b3bfac8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate predictions for training\n",
    "trainPredict = model.predict(train_x_cust,batch_size=1)\n",
    "testPredict = model.predict(test_x_cust,batch_size=1)\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(df_1015130)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[lag:len(trainPredict)+lag, :] = trainPredict\n",
    " \n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(df_1015130)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "#testPredictPlot[len(trainPredict)+(lag*2)+1:len(df_1015130)-1, :] = testPredict\n",
    " \n",
    "# plot baseline and predictions\n",
    "plt.plot(df_1015130['fv_cost'])\n",
    "plt.plot(trainPredictPlot)\n",
    "#plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions into csv\n",
    "result_prediction = pd.DataFrame()#, original_fv : test_y_cust, prediction_fv :testPredict})\n",
    "#result_prediction['dates'] = test_x['dates']\n",
    "result_prediction['original_fv'] = test_y_cust\n",
    "result_prediction['prediction_fv'] = testPredict\n",
    "\n",
    "result_prediction.to_csv(\"predictions_for_client_1015193.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 0.75878621  0.75815513  0.75752396  0.75689271  0.75626138  0.75562996\n  0.75562996  0.69999317  0.67483082  0.67415047  0.67347003  0.6727895\n  0.67210887  0.67142816  0.67074735  0.64380529  0.69311422  0.69244711\n  0.69177992  0.69111263  0.68518199  0.6845113   0.68384052  0.68316966\n  0.6824987   0.68182765  0.68115652  0.68048529  0.67981398  0.66689403\n  0.72207259  0.72142356  0.70814154  0.70748461  0.70682758  0.70617047\n  0.71043435  0.70978001  0.70912558  0.70847107  0.70781647  0.70716178\n  0.706507    0.70374683  0.70199974  0.69154391  0.69088083  0.69021765\n  0.68955439  0.68889104  0.75455957  0.75393566  0.75867825  0.7580574\n  0.75743647  0.75681546  0.75619437  0.75619437  0.72985868  0.72985868\n  0.59137962  0.58616359  0.58543926  0.61171779  0.61100945  0.61030101\n  0.60959248  0.76237582  0.76175953  0.76114315  0.7605267   0.75991016\n  0.75929353  0.75867683  0.75806004  0.75744317  0.75682622  0.75620918\n  0.75559207  0.75497487  0.75435758  0.75663275  0.75601694  0.75540106\n  0.75478508  0.74069988  0.78920383  0.78860425  0.78860425  0.77720869\n  0.77660231  0.77599586  0.77599586  0.53314188  0.53314188  0.52538153\n  0.52462222  0.5238628   0.5238628   0.4682062   0.46741297  0.46661964\n  0.4658262   0.46503266  0.4621337   0.46133866  0.4605435   0.45974825\n  0.45895288  0.45928781  0.4584929   0.36807329  0.36725906  0.33732543\n  0.33649314  0.33566075  0.33482825  0.28174626  0.26317491  0.26229924\n  0.27390471  0.27303491  0.27216499  0.27129495  0.2577919   0.17113483\n  0.1702035   0.16927205  0.16834048  0.16740878  0.17600358  0.17507677\n  0.17414984  0.17322279  0.17322279  0.17322279  0.16314669  0.15486466\n  0.15392713  0.15298948  0.1520517   0.14479788  0.14175068  0.14080737\n  0.13986393  0.13892037  0.13797668  0.13703287  0.12428273  0.12333144\n  0.11606445  0.11510903  0.11415349  0.11319782  0.11224202  0.11128609\n  0.11128609  0.0854486   0.0854486   0.04771918 -0.07866612 -0.07973707\n -0.08080816 -0.08187939 -0.08295077 -0.08402229 -0.19924733 -0.21568478\n -0.21683628 -0.21798793 -0.21798793 -0.23765362 -0.23881691 -0.23881691\n -0.23881691 -0.24849776 -0.24966658 -0.25083555 -0.25200468 -0.25317397\n -0.26169254 -0.26286663 -0.26404089 -0.2773415  -0.28120303 -0.99866323\n -1.         -1.         -0.94505322 -0.67814085 -0.64706264 -0.53400103\n -0.52989319 -0.53094811 -0.65739443 -0.66035272 -0.67411745 -0.67525857\n -0.68820427 -0.68935293 -0.69050173 -0.6916507  -0.42581015 -0.42683829\n -0.43276598 -0.4337974  -0.43482895 -0.43586064 -0.43689246 -0.44108239\n -0.44211642 -0.44100008 -0.4344243  -0.43545292 -0.43648167 -0.43751056\n -0.42576776 -0.42678926 -0.42781089 -0.42883266 -0.42985457 -0.43087661\n -0.43189879 -0.43292111 -0.43394357 -0.43394357 -0.57015625 -0.44214421\n -0.44317309 -0.4442021  -0.44523126 -0.43759969 -0.42658063 -0.42759787\n -0.43493116 -0.43595254 -0.43697405 -0.4379957  -0.43901749 -0.44003941\n -0.44106147 -0.44208366 -0.45290483 -0.4539333  -0.4549619  -0.45599065\n -0.01572572  0.70579033  0.96186564  0.94662146  0.94610674  1.\n  0.99951636  0.99903266  0.9985489   0.99806507  0.96806976  0.96756773\n  0.84167438  0.84109545  0.84558143  0.84500537  0.84127128  0.84069313\n  0.84011491  0.8395366   0.83580027  0.83416722  0.82378728  0.82320009\n  0.82261283  0.82202548  0.95931474  0.97220146  0.97170445  0.97120738\n  0.97071024  0.97021303  0.95501751  0.95451117  0.94137187  0.94085766\n  0.94625946].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6e598e62061e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y_cust\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scale_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 0.75878621  0.75815513  0.75752396  0.75689271  0.75626138  0.75562996\n  0.75562996  0.69999317  0.67483082  0.67415047  0.67347003  0.6727895\n  0.67210887  0.67142816  0.67074735  0.64380529  0.69311422  0.69244711\n  0.69177992  0.69111263  0.68518199  0.6845113   0.68384052  0.68316966\n  0.6824987   0.68182765  0.68115652  0.68048529  0.67981398  0.66689403\n  0.72207259  0.72142356  0.70814154  0.70748461  0.70682758  0.70617047\n  0.71043435  0.70978001  0.70912558  0.70847107  0.70781647  0.70716178\n  0.706507    0.70374683  0.70199974  0.69154391  0.69088083  0.69021765\n  0.68955439  0.68889104  0.75455957  0.75393566  0.75867825  0.7580574\n  0.75743647  0.75681546  0.75619437  0.75619437  0.72985868  0.72985868\n  0.59137962  0.58616359  0.58543926  0.61171779  0.61100945  0.61030101\n  0.60959248  0.76237582  0.76175953  0.76114315  0.7605267   0.75991016\n  0.75929353  0.75867683  0.75806004  0.75744317  0.75682622  0.75620918\n  0.75559207  0.75497487  0.75435758  0.75663275  0.75601694  0.75540106\n  0.75478508  0.74069988  0.78920383  0.78860425  0.78860425  0.77720869\n  0.77660231  0.77599586  0.77599586  0.53314188  0.53314188  0.52538153\n  0.52462222  0.5238628   0.5238628   0.4682062   0.46741297  0.46661964\n  0.4658262   0.46503266  0.4621337   0.46133866  0.4605435   0.45974825\n  0.45895288  0.45928781  0.4584929   0.36807329  0.36725906  0.33732543\n  0.33649314  0.33566075  0.33482825  0.28174626  0.26317491  0.26229924\n  0.27390471  0.27303491  0.27216499  0.27129495  0.2577919   0.17113483\n  0.1702035   0.16927205  0.16834048  0.16740878  0.17600358  0.17507677\n  0.17414984  0.17322279  0.17322279  0.17322279  0.16314669  0.15486466\n  0.15392713  0.15298948  0.1520517   0.14479788  0.14175068  0.14080737\n  0.13986393  0.13892037  0.13797668  0.13703287  0.12428273  0.12333144\n  0.11606445  0.11510903  0.11415349  0.11319782  0.11224202  0.11128609\n  0.11128609  0.0854486   0.0854486   0.04771918 -0.07866612 -0.07973707\n -0.08080816 -0.08187939 -0.08295077 -0.08402229 -0.19924733 -0.21568478\n -0.21683628 -0.21798793 -0.21798793 -0.23765362 -0.23881691 -0.23881691\n -0.23881691 -0.24849776 -0.24966658 -0.25083555 -0.25200468 -0.25317397\n -0.26169254 -0.26286663 -0.26404089 -0.2773415  -0.28120303 -0.99866323\n -1.         -1.         -0.94505322 -0.67814085 -0.64706264 -0.53400103\n -0.52989319 -0.53094811 -0.65739443 -0.66035272 -0.67411745 -0.67525857\n -0.68820427 -0.68935293 -0.69050173 -0.6916507  -0.42581015 -0.42683829\n -0.43276598 -0.4337974  -0.43482895 -0.43586064 -0.43689246 -0.44108239\n -0.44211642 -0.44100008 -0.4344243  -0.43545292 -0.43648167 -0.43751056\n -0.42576776 -0.42678926 -0.42781089 -0.42883266 -0.42985457 -0.43087661\n -0.43189879 -0.43292111 -0.43394357 -0.43394357 -0.57015625 -0.44214421\n -0.44317309 -0.4442021  -0.44523126 -0.43759969 -0.42658063 -0.42759787\n -0.43493116 -0.43595254 -0.43697405 -0.4379957  -0.43901749 -0.44003941\n -0.44106147 -0.44208366 -0.45290483 -0.4539333  -0.4549619  -0.45599065\n -0.01572572  0.70579033  0.96186564  0.94662146  0.94610674  1.\n  0.99951636  0.99903266  0.9985489   0.99806507  0.96806976  0.96756773\n  0.84167438  0.84109545  0.84558143  0.84500537  0.84127128  0.84069313\n  0.84011491  0.8395366   0.83580027  0.83416722  0.82378728  0.82320009\n  0.82261283  0.82202548  0.95931474  0.97220146  0.97170445  0.97120738\n  0.97071024  0.97021303  0.95501751  0.95451117  0.94137187  0.94085766\n  0.94625946].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "scaler.inverse_transform(test_y_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions for timesteps ahead\n",
    "# 7,14 ,21 timesteps ahead\n",
    "#we need atleast 4 elements to prepare training data\n",
    "futureElement = testPredict[-4:]\n",
    "\n",
    "def prepare_training_data_array(series_data, lag):\n",
    "    \" Converts a series of data into a lagged, scaled sample.\"\n",
    "    # scale training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    cost_vals = scaler.fit_transform(series_data.reshape(-1, 1))\n",
    "    \n",
    "    # convert series to lagged features\n",
    "    cost_lagged = lag_feature(cost_vals, lag=lag)\n",
    "\n",
    "    # X, y format taking the first column (original time series) to be the y\n",
    "    X = cost_lagged.drop('fv_cost', axis=1).values\n",
    "    y = cost_lagged.fv_cost.values\n",
    "    \n",
    "    # keras expects 3 dimensional X\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape)\n",
    "x=futureElement[:3]\n",
    "#def timestep_ahead(num,model,futureElement):\n",
    "#futureElement = futureElement_x\n",
    "def timestep_ahead(num,model,futureElement,x):\n",
    "    futureElements = []\n",
    "    futureElements.append(futureElement.tolist())\n",
    "\n",
    "    for i in range(num):\n",
    "        futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "        print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "        print(\"future element value after preparing data\",futureElement_x)\n",
    "        futureElement = model.predict(futureElement_x,batch_size=1,verbose=2)\n",
    "        futureElements.append(futureElement)\n",
    "        print(\"future element before appending x\",futureElement)\n",
    "        futureElement = np.append(futureElement,x)\n",
    "        x=futureElement[:3]\n",
    "        print(\"future element after appending x\",futureElement)\n",
    "        print(\"iteration \",i,\"  complete\")\n",
    "    return (futureElement[0],futureElements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(futureElements[0][0][0][:2])\n",
    "\n",
    "#future_x = np.array(futureElement,futureElements[0][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_7time_ahead,elements = timestep_ahead(7,model,futureElement,x)\n",
    "print (\"scaled prediction value on 7th day ahead \",_7time_ahead)\n",
    "_14time_ahead,elements  = timestep_ahead(14,model,futureElement,x)\n",
    "print (\"scaled prediction value on 14th day ahead \",_14time_ahead)\n",
    "_21time_ahead,elements  = timestep_ahead(21,model,futureElement,x)\n",
    "print (\"scaled prediction value on 21st day ahead \",_21time_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3months ahead predicitons\n",
    "_91days_ahead,elements = timestep_ahead(91,model,futureElement,x)\n",
    "print (\"scaled prediction value on 91st day ahead \",_91days_ahead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pred =[np.array(arr).tolist() for arr in elements]\n",
    "daily_pred\n",
    "def flatten(l): return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]\n",
    "pred_list = flatten(daily_pred)\n",
    "\n",
    "import csv \n",
    "myFile = open('daily_predictions_3months.csv', 'w')  \n",
    "with myFile:  \n",
    "   writer = csv.writer(myFile)\n",
    "   writer.writerow(pred_list)\n",
    "\n",
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find every 7th element into a list\n",
    "week_end_pred = pred_list[0::7]\n",
    "week_end_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly average for 3 months\n",
    "weekly_avg_pred = [(sum(pred_list[x:x+7]))/7 for x in range(0, len(pred_list), 7)]\n",
    "weekly_avg_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find average for every 7 days\n",
    "import itertools\n",
    "n=7 #7days\n",
    "list_week = list(itertools.chain.from_iterable([i]*7 for i in [sum(pred_list[i:i+7])//7 for i in range(0,len(pred_list),7)]))\n",
    "list_week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for i in range(7):\n",
    "futureElement = model.predict(futureElement,batch_size=1)\n",
    "futureElements.append(futureElement)\n",
    "futureElement = np.append(futureElement, futureElements[0][0][0][:3])\n",
    "x=futureElement[:3]\n",
    "print(x)\n",
    "print(\"iteration 0 complete\")\n",
    "#    #return futureElements\n",
    "print(futureElement)\n",
    "print(futureElements)\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "print(\"future element value after preparing data\",futureElement_x)\n",
    "futureElement = model.predict(futureElement_x,batch_size=1)\n",
    "futureElements.append(futureElement)\n",
    "print(\"future element before appending x\",futureElement)\n",
    "futureElement = np.append(futureElement,x)\n",
    "x=futureElement[:3]\n",
    "print(\"future element after appending x\",futureElement)\n",
    "print(\"iteration 1 complete\")\n",
    "print(futureElements)\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "print(\"future element value after preparing data\",futureElement_x)\n",
    "futureElement = model.predict(futureElement_x,batch_size=1)\n",
    "futureElements.append(futureElement)\n",
    "print(\"future element before appending x\",futureElement)\n",
    "futureElement = np.append(futureElement,x)\n",
    "x=futureElement[:3]\n",
    "print(\"future element after appending x\",futureElement)\n",
    "print(\"iteration 2 complete\")\n",
    "print(futureElements)\n",
    "\n",
    "print(type(futureElements))\n",
    "print(type(futureElement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [x for sublist in futureElements for x in sublist]\n",
    "print(flat_list[0][0][1])\n",
    "\n",
    "#above function loops in below format\n",
    "#for sublist in futureElements:\n",
    "#    for x in sublist:\n",
    "#        flat_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price.resample('1W')  \n",
    "#aapl.resample('W').mean()\n",
    "#http://benalexkeen.com/resampling-time-series-data-with-pandas/\n",
    "#resampling based on days and then slicing every 7th day\n",
    "#ts.resample('D').interpolate()[::7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.last('3M') #last 3 months data\n",
    "# to sort dates column before getting last 3months\n",
    "#df_sorted = df.sort_values(by=\"Date\",ascending=True) \\\n",
    "#    .set_index(\"Date\")\n",
    "#    .last(\"3M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/305863/how-to-train-lstm-model-on-multiple-time-series-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/67362/shall-i-use-weekly-or-monthly-data-for-forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weekly predicitons\n",
    "#weekly data\n",
    "week_data = df_1015130.set_index('dates').resample('1W').mean()\n",
    "#week_data['dates'] = pd.to_datetime(week_data['dates'], errors='coerce')\n",
    "\n",
    "\n",
    "week_data = week_data.resample('1W')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#week_data['dates'] = pd.to_datetime(week_data['dates'], unit='D',utc=True)\n",
    "week_data = df_1015130\n",
    "##week_data['New']=week_data.dates.map(week_data.set_index('dates').iloc[1:].resample('D').sum().rolling(7,min_periods =1).visit.mean()).shift()\n",
    "week_data['dates'] = pd.to_datetime(week_data['dates'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data.set_index('dates',inplace=True)\n",
    "week_data_resample = week_data.resample('1W').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data_resample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_index = week_data_resample.index(\"/2018\")\n",
    "#l2 = l[:c_index]\n",
    "week_data_resample['dates'] = week_data_resample.index\n",
    "week_data_resample['dates']=week_data_resample['dates'].astype(str)\n",
    "week_data_resample = week_data_resample.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_week = week_data_resample[week_data_resample['dates'].str.contains('2018') == False]\n",
    "test_week = week_data_resample[week_data_resample['dates'].str.contains('2018') == True]\n",
    "\n",
    "train_x_week,train_y_week, scaler = prepare_training_data(train_week['fv_cost'], 3)\n",
    "test_x_week,test_y_week,scalar = prepare_training_data(test_week['fv_cost'], 3)\n",
    "print(train_x_week.shape)\n",
    "print(train_y_week.shape)\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag =  3\n",
    "num_neurons = 24\n",
    "batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "batch_input_shape=(batch_size, 1, lag)\n",
    "\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=24,batch_size=1, kernel_size=3, strides=3, padding=\"same\",activation='relu',dilation_rate=1, input_shape=(1, 3),data_format='channels_first'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x_week, train_y_week, epochs=10, batch_size=batch_size, verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainScore = model.evaluate(train_x_week, train_y_week, batch_size=1, verbose=2)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(test_x_week, test_y_week, batch_size=1, verbose=2)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))\n",
    "print('unscaled test score %f MSE' %(scaler.inverse_transform(testScore)))\n",
    "scaler.inverse_transform(trainScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for training\n",
    "trainPredict = model.predict(train_x_week,batch_size=1)\n",
    "testPredict = model.predict(test_x_week,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futureElement = testPredict[-4:]\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape)\n",
    "x=futureElement[:3]\n",
    "#def timestep_ahead(num,model,futureElement):\n",
    "#futureElement = futureElement_x\n",
    "def timestep_ahead_week(num,model,futureElement,x):\n",
    "    futureElements = []\n",
    "    futureElements.append(futureElement.tolist())\n",
    "\n",
    "    for i in range(num):\n",
    "        futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "        print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "        print(\"future element value after preparing data\",futureElement_x)\n",
    "        futureElement = model.predict(futureElement_x,batch_size=1,verbose=2)\n",
    "        futureElements.append(futureElement)\n",
    "        print(\"future element before appending x\",futureElement)\n",
    "        futureElement = np.append(futureElement,x)\n",
    "        x=futureElement[:3]\n",
    "        print(\"future element after appending x\",futureElement)\n",
    "        print(\"iteration \",i,\"  complete\")\n",
    "    return (futureElement[0],futureElements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_12_weeks_ahead,all_pred = timestep_ahead_week(12,model,futureElement,x)\n",
    "print (\"scaled prediction value on 3rd month ahead \",_12_weeks_ahead)\n",
    "print (\"all predicitons \",all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of arrays to lists\n",
    "#l = [arr.tolist() for arr in l]\n",
    "#[l.tolist() for l in list1]\n",
    "\n",
    "#weekly_pred = [arr.tolist() for arr in all_pred] #list object has no attribute tolist\n",
    "weekly_pred =[np.array(arr).tolist() for arr in all_pred]\n",
    "weekly_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l): return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = flatten(weekly_pred)\n",
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvwriter.writerow(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "myFile = open('weekly_predictions_3months.csv', 'w')  \n",
    "with myFile:  \n",
    "   writer = csv.writer(myFile)\n",
    "   writer.writerow(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
