{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This notebook is intended to experiment the model on one customer with \n",
    "# model parameters to get best optimised model\n",
    "#Loading the libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten    \n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras import optimizers\n",
    "\n",
    "# Reading csv file\n",
    "#customers timeseries updated has data for 100 customers\n",
    "#df = pd.read_csv(\"customers_timeseries_updated.csv\")\n",
    "# part-00000-581ed144-c0f2-45b7-b743-a35fb688f431-c000.csv dataset has \n",
    "# data for 1000 customers\n",
    "df = pd.read_csv(\"part-00000-581ed144-c0f2-45b7-b743-a35fb688f431-c000.csv\")\n",
    "#only using required columns for modelling\n",
    "df = df[['client_debtor_number','dates','fv_cost']]\n",
    "# number of unique customers in the data\n",
    "len(df['client_debtor_number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting one customer data from the set\n",
    "# 1015193 is customer from cluster 1 with 130 customers\n",
    "#df_1015130 = df[df['client_debtor_number'] == 1015193]\n",
    "# 816580 is customer \n",
    "df_cust = df[df['client_debtor_number'] == 8165580]\n",
    "del df_cust['client_debtor_number']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting train and test datasets \n",
    "#training 2010-2017 years data, testing on 2018 data\\\n",
    "train_x = df_cust[df_cust['dates'].str.contains('/2018') == False]\n",
    "test_x = df_cust[df_cust['dates'].str.contains('/2018') == True]\n",
    "train_x.reset_index(drop=True,inplace=True)\n",
    "test_x.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1017, 1, 3)\n",
      "(1017,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# lag_feature function is used to get new column in the dataframe with \n",
    "#lagged data,number of lags can be given as function parameter \n",
    "def lag_feature(df, lag=1):\n",
    "    if not type(df) == pd.DataFrame:\n",
    "        df = pd.DataFrame(df, columns=['fv_cost'])\n",
    "    \n",
    "    def rename_lag(ser, j):\n",
    "        ser.name = ser.name + f'_{j}'\n",
    "        return ser\n",
    "        \n",
    "    # add a column lagged by `i` steps\n",
    "    for i in range(1, lag + 1):\n",
    "        df = df.join(df.fv_cost.shift(i).pipe(rename_lag, i))\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Prepare training data function is used to scale the fv_cost values \n",
    "# between -1 to 1 and calls lag_feature to create lag columns \n",
    "def prepare_training_data(series_data, lag):\n",
    "    \" Converts a series of data into a lagged, scaled sample.\"\n",
    "    # scale training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #cost_vals = scaler.fit_transform(series_data.values.reshape(-1, 1))\n",
    "    cost_vals = scaler.fit_transform(series_data.values.reshape(-1, 1))\n",
    "    # convert series to lagged features\n",
    "    cost_lagged = lag_feature(cost_vals, lag=lag)\n",
    "\n",
    "    # X, y format taking the first column (original time series) to be the y\n",
    "    X = cost_lagged.drop('fv_cost', axis=1).values\n",
    "    y = cost_lagged.fv_cost.values\n",
    "    \n",
    "    # keras expects 3 dimensional X\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "\n",
    "#sorting train and test data based on dates column values\n",
    "train_x['dates'] = pd.to_datetime(train_x['dates'],format= '%d/%m/%Y')\n",
    "train_x.sort_values(by='dates',inplace=True)\n",
    "test_x['dates'] = pd.to_datetime(test_x['dates'],format= '%d/%m/%Y')\n",
    "test_x.sort_values(by='dates',inplace=True)\n",
    "\n",
    "# create train and test datasets in a format required by the model\n",
    "train_x_cust,train_y_cust, scalar_train = prepare_training_data(train_x['fv_cost'], 3)\n",
    "test_x_cust,test_y_cust,scalar_test = prepare_training_data(test_x['fv_cost'], 3)\n",
    "print(train_x_cust.shape)\n",
    "print(train_y_cust.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model description\n",
    "# lag is number of lags used to prepare the data\n",
    "lag =  3\n",
    "\n",
    "# model parameters\n",
    "num_neurons = 50 #number of neurons/nodes for the layer\n",
    "# actually we can give data in batches to the model\n",
    "# number of samples in the data should be divisible by batch size\n",
    "# we are giving all data for a customer as one batch\n",
    "batch_size = 1  \n",
    "# input_shape as required by LSTM layer\n",
    "batch_input_shape=(batch_size, 1, lag)\n",
    "# dropout rate used in dropout layer\n",
    "dropout_rate =0.2\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "#add convolution layer\n",
    "# input_shape=(3,1) 3 lags columns and 1 y value\n",
    "# when strides>1 you cannot have dilation>1\n",
    "# activation_func options ['softmax', 'softplus', 'softsign', 'relu', \n",
    "#                   'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "# in conv1D layer we can modify filters,kernel_size,strides and activation\n",
    "# parameters\n",
    "model.add(Conv1D(filters=num_neurons,batch_size=1, kernel_size=3,\n",
    "                 strides=3, \n",
    "                 padding=\"same\",activation='linear',dilation_rate=1, \n",
    "                 input_shape=(1, 3),data_format='channels_first'))\n",
    "# maxpooling layers tries to reduce the features by taking maximum value for\n",
    "# window/pool size selected,strides and pool_size can be changed\n",
    "model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "model.add(Dropout(dropout_rate))\n",
    "# add LSTM layer - stateful MUST be true here in \n",
    "# order to learn the patterns within a series\n",
    "model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "\n",
    "# followed by a dense layer with a single output for regression\n",
    "#model.add(Dense(16,activation='linear'))\n",
    "#model.add(Dense(18,activation='exponential'))\n",
    "model.add(Dense(1,activation='linear'))\n",
    "# we can add dropoutlayer after dense as well again\n",
    "#optimizer function options \n",
    "#['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "adam = optimizers.Adam(lr=0.01, decay=0.01) \n",
    "sgd = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.01, nesterov=False)\n",
    "adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "#compile the model\n",
    "# give the name directly if you want to use function with default parameters\n",
    "model.compile(loss='mean_squared_error', optimizer='RMSprop')\n",
    "#if you want manual edited parameters you can use below code\n",
    "#adam = optimizers.Adam(lr=0.01, decay=0.01)\n",
    "#model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 18s - loss: 0.0182\n",
      "Epoch 2/10\n",
      " - 14s - loss: 0.0161\n",
      "Epoch 3/10\n",
      " - 14s - loss: 0.0164\n",
      "Epoch 4/10\n",
      " - 14s - loss: 0.0169\n",
      "Epoch 5/10\n",
      " - 14s - loss: 0.0166\n",
      "Epoch 6/10\n",
      " - 14s - loss: 0.0167\n",
      "Epoch 7/10\n",
      " - 14s - loss: 0.0165\n",
      "Epoch 8/10\n",
      " - 14s - loss: 0.0169\n",
      "Epoch 9/10\n",
      " - 14s - loss: 0.0173\n",
      "Epoch 10/10\n",
      " - 14s - loss: 0.0219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a400d6ba8>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(train_x_cust, train_y_cust, epochs=10, batch_size=batch_size, verbose=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.04 MSE (0.19 RMSE)\n",
      "Test Score: 0.19 MSE (0.44 RMSE)\n",
      "scaled test rmse  0.438323752274294\n",
      "scaled train rmse  0.19371055505121876\n",
      "actual test rmse  0.219161876137147\n",
      "actual train rmse  1.3858194970365958\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "# default batch size is 32 so we need to give batch size explicitly \n",
    "# if we want different batch size or \n",
    "# input size should be divisible by batch size for stateful LSTM\n",
    "trainScore = model.evaluate(train_x_cust,train_y_cust,batch_size=1,verbose=2)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % \n",
    "      (trainScore, sqrt(trainScore)))\n",
    "testScore = model.evaluate(test_x_cust,test_y_cust,batch_size=1,verbose=2)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % \n",
    "      (testScore,sqrt(testScore)))\n",
    "\n",
    "# Model Predicitions\n",
    "trainPredict = model.predict(train_x_cust,batch_size=1)\n",
    "testPredict = model.predict(test_x_cust,batch_size=1)\n",
    "# use these values when difference lag values are used for model\n",
    "#yhat_act_test = [scalar_test.inverse_transform(testPredict)[i-lag]+diff_test[i]\n",
    "#                 for i in range(lag,len(diff_test))]\n",
    "\n",
    "#yhat_act_train = [scalar_train.inverse_transform(trainPredict)[i-lag]+diff_train[i]\n",
    "#                  for i in range(lag,len(diff_train))]\n",
    "\n",
    "# Getting inverse scaled values for the predicted values \n",
    "yhat_act_test = scalar_test.inverse_transform(testPredict)\n",
    "yhat_act_train = scalar_train.inverse_transform(trainPredict)\n",
    "print(\"scaled test rmse \",sqrt(mean_squared_error(test_y_cust,testPredict)))\n",
    "print(\"scaled train rmse \",sqrt(mean_squared_error(train_y_cust,trainPredict)))\n",
    "print (\"actual test rmse \",sqrt(mean_squared_error(test_x.fv_cost[-(len(yhat_act_test)):], yhat_act_test)))\n",
    "print (\"actual train rmse \",sqrt(mean_squared_error(train_x.fv_cost[-(len(yhat_act_train)):],yhat_act_train)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra code to use difference values\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        #cannot take log here because of NAN's errors\n",
    "        value = (dataset[i] - dataset[i - interval])/(1+abs(dataset[i - interval]))\n",
    "        diff.append(value)\n",
    "    return diff\n",
    " \n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, value):\n",
    "    return value + last_ob\n",
    "\n",
    "#sorting data based on dates\n",
    "train_x['dates'] = pd.to_datetime(train_x['dates'],format= '%d/%m/%Y')\n",
    "train_x.sort_values(by='dates',inplace=True)\n",
    "test_x['dates'] = pd.to_datetime(test_x['dates'],format= '%d/%m/%Y')\n",
    "test_x.sort_values(by='dates',inplace=True)\n",
    "\n",
    "#diff_train = np.array(difference(train_x['fv_cost']))\n",
    "#diff_test = np.array(difference(test_x['fv_cost']))\n",
    "#train_x_cust,train_y_cust, scalar_train = prepare_training_data(diff_train, 3)\n",
    "#test_x_cust,test_y_cust,scalar_test = prepare_training_data(diff_test, 3)\n",
    "train_x_cust,train_y_cust, scalar_train = prepare_training_data(train_x['fv_cost'], 3)\n",
    "test_x_cust,test_y_cust,scalar_test = prepare_training_data(test_x['fv_cost'], 3)\n",
    "\n",
    "print(train_x_cust.shape)\n",
    "print(train_y_cust.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch CV experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Function to create model, required for KerasRegressor\n",
    "def create_model(optimizer='adam',strides_num =3,kernel_size =3,num_neurons=1,\n",
    "                 activation_func ='relu',dropout_rate = 0.2):\n",
    "    lag =  3\n",
    "    num_neurons = 24\n",
    "    batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "    batch_input_shape=(batch_size, 1, lag)\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=num_neurons,batch_size=1, kernel_size=kernel_size, strides=strides_num, \n",
    "                     padding=\"same\",activation=activation_func,dilation_rate=1, input_shape=(1, 3),\n",
    "                     data_format='channels_first'))\n",
    "    model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "\n",
    "    # followed by a dense layer with a single output for regression\n",
    "    model.add(Dense(1))\n",
    "    # we can add dropoutlayer after dense as well again\n",
    "    model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=2,batch_size = 1,epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-416a07354e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_cust\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_cust\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#activation_func = ['softmax', 'softplus', 'softsign', 'relu', \n",
    "#                   'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "activation_func = ['softmax', 'relu', \n",
    "                   'tanh', 'sigmoid', 'linear']\n",
    "#param_grid = dict(optimizer=optimizer,activation_func = activation_func)\n",
    "param_grid = dict(activation_func = activation_func)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(train_x_cust, train_y_cust)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FORECAST BIAS\n",
    "forecast_errors = [expected[i]-predictions[i] for i in range(len(expected))]\n",
    "bias = sum(forecast_errors) * 1.0/len(expected)\n",
    "print('Bias: %f' % bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions into csv\n",
    "result_prediction = pd.DataFrame()#, original_fv : test_y_cust, prediction_fv :testPredict})\n",
    "#result_prediction['dates'] = test_x['dates']\n",
    "result_prediction['original_fv'] = test_y_cust\n",
    "result_prediction['prediction_fv'] = testPredict\n",
    "\n",
    "result_prediction.to_csv(\"predictions_for_client_1015193.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly average for 3 months\n",
    "weekly_avg_pred = [(sum(pred_list[x:x+7]))/7 for x in range(0, len(pred_list), 7)]\n",
    "weekly_avg_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/305863/how-to-train-lstm-model-on-multiple-time-series-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/67362/shall-i-use-weekly-or-monthly-data-for-forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weekly predicitons\n",
    "#weekly data\n",
    "week_data = df_1015130.set_index('dates').resample('1W').mean()\n",
    "#week_data['dates'] = pd.to_datetime(week_data['dates'], errors='coerce')\n",
    "\n",
    "week_data = week_data.resample('1W')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#week_data['dates'] = pd.to_datetime(week_data['dates'], unit='D',utc=True)\n",
    "week_data = df_1015130\n",
    "##week_data['New']=week_data.dates.map(week_data.set_index('dates').iloc[1:].resample('D').sum().rolling(7,min_periods =1).visit.mean()).shift()\n",
    "week_data['dates'] = pd.to_datetime(week_data['dates'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data.set_index('dates',inplace=True)\n",
    "week_data_resample = week_data.resample('1W').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data_resample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_index = week_data_resample.index(\"/2018\")\n",
    "#l2 = l[:c_index]\n",
    "week_data_resample['dates'] = week_data_resample.index\n",
    "week_data_resample['dates']=week_data_resample['dates'].astype(str)\n",
    "week_data_resample = week_data_resample.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_week = week_data_resample[week_data_resample['dates'].str.contains('2018') == False]\n",
    "test_week = week_data_resample[week_data_resample['dates'].str.contains('2018') == True]\n",
    "\n",
    "train_x_week,train_y_week, scalar_train = prepare_training_data(train_week['fv_cost'], 3)\n",
    "test_x_week,test_y_week,scalar_test = prepare_training_data(test_week['fv_cost'], 3)\n",
    "print(train_x_week.shape)\n",
    "print(train_y_week.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_12_weeks_ahead,all_pred = timestep_ahead_week(12,model,futureElement,x)\n",
    "print (\"scaled prediction value on 3rd month ahead \",_12_weeks_ahead)\n",
    "print (\"all predicitons \",all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of arrays to lists\n",
    "#l = [arr.tolist() for arr in l]\n",
    "#[l.tolist() for l in list1]\n",
    "\n",
    "#weekly_pred = [arr.tolist() for arr in all_pred] #list object has no attribute tolist\n",
    "weekly_pred =[np.array(arr).tolist() for arr in all_pred]\n",
    "weekly_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
