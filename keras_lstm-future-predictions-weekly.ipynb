{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This notebook is to get Future Predicted values\n",
    "#Loading the libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout   \n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras import optimizers\n",
    "\n",
    "# Reading csv file\n",
    "#customers timeseries updated has data for 100 customers\n",
    "df = pd.read_csv(\"customers_timeseries_updated.csv\")\n",
    "#only using required columns for modelling\n",
    "df = df[['client_debtor_number','dates','fv_cost']]\n",
    "# number of unique customers in the data\n",
    "len(df['client_debtor_number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        dates  fv_cost\n",
      "0  10/07/2010      0.0\n",
      "1  11/07/2010      0.0\n",
      "2  12/07/2010      0.0\n",
      "3  13/07/2010      0.0\n",
      "4  14/07/2010      0.0\n"
     ]
    }
   ],
   "source": [
    "# subsetting one customer data from the set\n",
    "df_1015130 = df[df['client_debtor_number'] == 1015193]\n",
    "del df_1015130['client_debtor_number']\n",
    "print(df_1015130.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# splitting train and test datasets \n",
    "#df_1015130['dates'] = df_1015130.index.values\n",
    "df_1015130['dates'] = pd.to_datetime(df_1015130['dates'],format= '%d/%m/%Y')\n",
    "#weekly data\n",
    "week_data = df_1015130.set_index('dates').resample('1W').mean()\n",
    "week_data['dates'] = week_data.index.values\n",
    "#training 2010-2017, testing on 2018 data\\\n",
    "#train_x = week_data[week_data['dates'].str.contains('/2018') == False]\n",
    "#test_x = week_data[week_data['dates'].str.contains('/2018') == True]\n",
    "train_x = week_data[~(week_data['dates'].dt.year == 2018)]\n",
    "test_x = week_data[(week_data['dates'].dt.year == 2018)]\n",
    "#resetting index values\n",
    "train_x.reset_index(drop=True,inplace=True)\n",
    "test_x.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#sorting data based on dates\n",
    "train_x['dates'] = pd.to_datetime(train_x['dates'],format= '%d/%m/%Y')\n",
    "train_x.sort_values(by='dates',inplace=True)\n",
    "test_x['dates'] = pd.to_datetime(test_x['dates'],format= '%d/%m/%Y')\n",
    "test_x.sort_values(by='dates',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415, 1, 3)\n",
      "(415,)\n"
     ]
    }
   ],
   "source": [
    "# lag_feature function is used to get new column in the dataframe with \n",
    "#lagged data,number of lags can be given as function parameter \n",
    "def lag_feature(df, lag=1):\n",
    "    if not type(df) == pd.DataFrame:\n",
    "        df = pd.DataFrame(df, columns=['fv_cost'])   \n",
    "    def rename_lag(ser, j):\n",
    "        ser.name = ser.name + f'_{j}'\n",
    "        return ser        \n",
    "    # add a column lagged by `i` steps\n",
    "    for i in range(1, lag + 1):\n",
    "        df = df.join(df.fv_cost.shift(i).pipe(rename_lag, i))\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Prepare training data function is used to scale the fv_cost values \n",
    "# between -1 to 1 and calls lag_feature to create lag columns \n",
    "def prepare_training_data(series_data, lag):\n",
    "    \" Converts a series of data into a lagged, scaled sample.\"\n",
    "    # scale training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #when diff is used don't use values\n",
    "    cost_vals = scaler.fit_transform(series_data.values.reshape(-1, 1))   \n",
    "    #cost_vals = scaler.fit_transform(series_data.reshape(-1, 1))\n",
    "    # convert series to lagged features\n",
    "    cost_lagged = lag_feature(cost_vals, lag=lag)\n",
    "    # X, y format taking the first column (original time series) to be the y\n",
    "    X = cost_lagged.drop('fv_cost', axis=1).values\n",
    "    y = cost_lagged.fv_cost.values\n",
    "    \n",
    "    # keras expects 3 dimensional X\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])   \n",
    "    return X, y, scaler\n",
    "\n",
    "#Difference function is get lag difference values\n",
    "def difference(dataset):\n",
    "    interval=1\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        #print (\"iteration \",i)\n",
    "        value = (dataset.iloc[i] - dataset.iloc[i - interval])/(1+abs(dataset.iloc[i - interval]))\n",
    "        diff.append(value)\n",
    "    return diff\n",
    "\n",
    "# if you are using difference lag values use this code\n",
    "#diff_train = np.array(difference(train_x['fv_cost']))\n",
    "#diff_test = np.array(difference(test_x['fv_cost']))\n",
    "#train_x_cust,train_y_cust, scalar_train = prepare_training_data(diff_train, 3)\n",
    "#test_x_cust,test_y_cust,scalar_test = prepare_training_data(diff_test, 3)\n",
    "# preparing data on train and test fv_cost column values\n",
    "train_x_cust,train_y_cust, scalar_train = prepare_training_data(train_x['fv_cost'], 3)\n",
    "test_x_cust,test_y_cust,scalar_test = prepare_training_data(test_x['fv_cost'], 3)\n",
    "print(train_x_cust.shape)\n",
    "print(train_y_cust.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model description\n",
    "# lag is number of lags used to prepare the data\n",
    "lag =  3\n",
    "# model parameters\n",
    "num_neurons = 50 #number of neurons/nodes for the layer\n",
    "# actually we can give data in batches to the model\n",
    "# number of samples in the data should be divisible by batch size\n",
    "# we are giving all data for a customer as one batch\n",
    "batch_size = 1  \n",
    "# input_shape as required by LSTM layer\n",
    "batch_input_shape=(batch_size, 1, lag)\n",
    "# dropout rate used in dropout layer\n",
    "dropout_rate =0.2\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "#add convolution layer\n",
    "# input_shape=(3,1) 3 lags columns and 1 y value\n",
    "# when strides>1 you cannot have dilation>1\n",
    "# activation_func options ['softmax', 'softplus', 'softsign', 'relu', \n",
    "#                   'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "# in conv1D layer we can modify filters,kernel_size,strides and activation\n",
    "# parameters\n",
    "model.add(Conv1D(filters=num_neurons,batch_size=1, kernel_size=3, strides=3, \n",
    "                 padding=\"same\",activation='linear',dilation_rate=1, \n",
    "                 input_shape=(1, 3),data_format='channels_first'))\n",
    "# maxpooling layers tries to reduce the features by taking maximum value for\n",
    "# window/pool size selected,strides and pool_size can be changed\n",
    "model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "model.add(Dropout(dropout_rate))\n",
    "# add LSTM layer - stateful MUST be true here in \n",
    "# order to learn the patterns within a series\n",
    "model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "# output layer\n",
    "model.add(Dense(1,activation='linear'))\n",
    "# compile the model with required loss and optimizer function\n",
    "adam = optimizers.Adam(lr=0.01, decay=0.01)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 5s - loss: 0.0519\n",
      "Epoch 2/10\n",
      " - 4s - loss: 0.0653\n",
      "Epoch 3/10\n",
      " - 4s - loss: 0.0290\n",
      "Epoch 4/10\n",
      " - 4s - loss: 0.0258\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.0156\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.0128\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.0149\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.0124\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.0121\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.0116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2b243ba8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "model.fit(train_x_cust, train_y_cust, epochs=10, batch_size=batch_size, verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.02 MSE (0.15 RMSE)\n",
      "Test Score: 0.08 MSE (0.28 RMSE)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=0.07928849593710968.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-87de190f99f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Score: %.2f MSE (%.2f RMSE)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtestScore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestScore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unscaled test score %f MSE'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestScore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0myhat_act_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscalar_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestScore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0myhat_act_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscalar_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainScore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0;32m--> 413\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    543\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    546\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=0.07928849593710968.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "# default batch size is 32 so we need to give batch size explicitly if we want different batch size or \n",
    "# input size should be divisible by batch size for stateful LSTM\n",
    "trainScore = model.evaluate(train_x_cust, train_y_cust, batch_size=1, verbose=2)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, sqrt(trainScore)))\n",
    "testScore = model.evaluate(test_x_cust, test_y_cust, batch_size=1, verbose=2)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, sqrt(testScore)))\n",
    "\n",
    "print('unscaled test score %f MSE' %(scalar_test.inverse_transform(testScore)))\n",
    "yhat_act_test = scalar_test.inverse_transform(testScore)\n",
    "yhat_act_train = scalar_train.inverse_transform(trainScore)\n",
    "print (\"actual test rmse \",sqrt(mean_squared_error(test_x.fv_cost[-(len(yhat_act_test)):], yhat_act_test)))\n",
    "print (\"actual train rmse \",sqrt(mean_squared_error(train_x.fv_cost[-(len(yhat_act_train)):],yhat_act_train)))\n",
    "#scaler.inverse_transform(testScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions of model\n",
    "trainPredict = model.predict(train_x_cust,batch_size=1)\n",
    "testPredict = model.predict(test_x_cust,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions into csv\n",
    "result_prediction = pd.DataFrame()#, original_fv : test_y_cust, prediction_fv :testPredict})\n",
    "#result_prediction['dates'] = test_x['dates']\n",
    "result_prediction['original_fv'] = test_y_cust\n",
    "result_prediction['prediction_fv'] = testPredict\n",
    "\n",
    "result_prediction.to_csv(\"predictions_for_client_1015193.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation on Future values of train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions for timesteps ahead\n",
    "# 7,14 ,21 timesteps ahead\n",
    "#we need atleast 4 elements to prepare training data\n",
    "#futureElement = testPredict[-4:]\n",
    "# if using testpredict as futureElement no need to scale data again using MinMaxscalar\n",
    "futureElement = train_x.fv_cost[-4:]\n",
    "x=futureElement[-3:]\n",
    "x=x.reshape(-1, 1)\n",
    "x.reshape(x.shape[1], 1, x.shape[0])\n",
    "\n",
    "def timestep_ahead(num,model,futureElement,x):\n",
    "    #Since we are using train we need to scale the input\n",
    "    futureElement_x,futureElement_y,scalar = prepare_training_data(futureElement[-4:], 3)\n",
    "    #print (\"shape of fe_x before for loop \",futureElement_x.shape)\n",
    "    futureElements = []\n",
    "    unscaled_prediction =[]\n",
    "    futureElements.append(futureElement.tolist())\n",
    "    # plus 1 because one value for futureElement_y \n",
    "    for i in range(num+1):\n",
    "       # print(futureElement_x.shape,\"shape after preparing data in \",i,\" iteration\")\n",
    "        #print(\"future element value after preparing data\",futureElement_x)\n",
    "        futureElement = model.predict(futureElement_x,batch_size=1,verbose=2)\n",
    "        unscaled_prediction.append(scalar.inverse_transform(futureElement).tolist())\n",
    "        #print(\"unscaled prediction \",unscaled_prediction)\n",
    "        futureElements.append(futureElement.tolist())\n",
    "        #print(\"future element before appending x\",futureElement)\n",
    "        futureElement = np.append(x,futureElement)\n",
    "        x=futureElement[-3:]\n",
    "        futureElement_x=x.reshape(-1, 1)\n",
    "        futureElement_x=futureElement_x.reshape(\n",
    "            futureElement_x.shape[1], 1, futureElement_x.shape[0])\n",
    "       # print(\"future element after appending x\",futureElement)\n",
    "        #print(\"futureElements \",futureElements)     \n",
    "        #print(\"iteration \",i,\"  complete\")     \n",
    "    return (futureElement[-1],futureElements,unscaled_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing future elements of train data with test dataset to check model performance\n",
    "# change the number to get number of predictions ahead\n",
    "_7ahead,elements,unscaled_prediction = timestep_ahead(7,model,train_x.fv_cost[-4:],x)\n",
    "print (\"prediction of 7days ahead \",_7ahead,\"scaled actual value\",test_y_cust[7],\" unscaled prediction \",unscaled_prediction[-1],\" actual value \",test_x.fv_cost.iloc[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_14ahead,elements,unscaled_prediction = timestep_ahead(14,model,train_x.fv_cost[-4:],x)\n",
    "print (\"prediction of 14days ahead \",_14ahead,\"scaled actual value\",test_y_cust[14],\" unscaled prediction \",unscaled_prediction[-1],\" actual value \",test_x.fv_cost.iloc[14])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_21ahead,elements,unscaled_prediction = timestep_ahead(21,model,train_x.fv_cost[-4:],x)\n",
    "print (\"prediction of 21days ahead \",_21ahead,\"scaled actual value\",\n",
    "       test_y_cust[21],\" unscaled prediction \",\n",
    "       unscaled_prediction[-1],\" actual value \",test_x.fv_cost.iloc[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_28ahead,elements,unscaled_prediction = timestep_ahead(28,model,train_x.fv_cost[-4:],x)\n",
    "print (\"prediction of 28days ahead \",_28ahead,\"scaled actual value\",\n",
    "       test_y_cust[28],\" unscaled prediction \",\n",
    "       unscaled_prediction[-1],\" actual value \",test_x.fv_cost.iloc[28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_90ahead,elements,unscaled_prediction = timestep_ahead(90,model,train_x.fv_cost[-4:],x)\n",
    "print (\"prediction of 90days ahead \",_90ahead,\"scaled actual value\",\n",
    "       test_y_cust[90],\" unscaled prediction \",\n",
    "       unscaled_prediction[-1],\" actual value \",test_x.fv_cost.iloc[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
