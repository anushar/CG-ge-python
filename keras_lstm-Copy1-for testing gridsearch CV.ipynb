{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/anusha/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras_timeseries.py\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten    \n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "#from keras.layers.convolutional import Convolution2D\n",
    "#from keras.layers.convolutional import MaxPooling2D\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from keras import backend as K\n",
    "\n",
    "df = pd.read_csv(\"customers_timeseries_updated.csv\")\n",
    "#df = pd.read_csv(\"data-1000customers.csv\")\n",
    "#df = pd.read_csv(\"100_timeseries_checking_to_share.csv\")\n",
    "#print (df.columns.values)\n",
    "#df = df.iloc[:,0:3]\n",
    "#use column names\n",
    "df = df[['client_debtor_number','dates','fv_cost']]\n",
    "#print(\"new dataset columns \",df.columns.values)\n",
    "#df.head()\n",
    "# number of customers\n",
    "len(df['client_debtor_number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1015130 = df[df['client_debtor_number'] == 1015193]\n",
    "del df_1015130['client_debtor_number']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data  0           0.000000\n",
      "1           0.000000\n",
      "2           0.000000\n",
      "3           0.000000\n",
      "4           0.000000\n",
      "5           0.000000\n",
      "6           0.000000\n",
      "7           0.000000\n",
      "8           0.000000\n",
      "9           0.000000\n",
      "10          0.000000\n",
      "11          0.000000\n",
      "12          0.000000\n",
      "13          0.000000\n",
      "14          0.000000\n",
      "15          0.000000\n",
      "16          0.000000\n",
      "17          0.000000\n",
      "18          0.000000\n",
      "19          0.000000\n",
      "20          0.000000\n",
      "21          0.000000\n",
      "22          0.000000\n",
      "23          0.000000\n",
      "24          0.000000\n",
      "25          0.000000\n",
      "26          0.000000\n",
      "27          0.000000\n",
      "28          0.000000\n",
      "29          0.000000\n",
      "             ...    \n",
      "307581    190.301116\n",
      "307582    193.302938\n",
      "307583    196.305162\n",
      "307584    197.021261\n",
      "307585    199.928856\n",
      "307586    191.000154\n",
      "307587    194.007212\n",
      "307588    194.524591\n",
      "307589    197.375700\n",
      "307590    200.080716\n",
      "307591     61.073114\n",
      "307592     62.782689\n",
      "307593     64.492494\n",
      "307594     66.202527\n",
      "307595     67.912788\n",
      "307596     67.921500\n",
      "307597     70.007468\n",
      "307598     71.637040\n",
      "307599     71.879163\n",
      "307600     73.556932\n",
      "307601    351.494657\n",
      "307602    354.894033\n",
      "307603    358.293864\n",
      "307604    361.425029\n",
      "307605    364.835032\n",
      "307606    368.245490\n",
      "307607    371.656404\n",
      "307608    375.067774\n",
      "307609    378.479600\n",
      "307610    381.891882\n",
      "Name: fv_cost, Length: 3192, dtype: float64\n",
      "test_data  152       392.131466\n",
      "153       381.770362\n",
      "154       385.427760\n",
      "155       389.324922\n",
      "156       393.222606\n",
      "157       397.120811\n",
      "158       401.019537\n",
      "159       404.918784\n",
      "160       788.531212\n",
      "161       794.142636\n",
      "162       801.190424\n",
      "163       806.942346\n",
      "164       814.036652\n",
      "165       821.131907\n",
      "166       828.228111\n",
      "167       782.594663\n",
      "168       788.944640\n",
      "169       794.757227\n",
      "16340     408.818552\n",
      "16341     408.818552\n",
      "16342     406.676080\n",
      "16343     408.093676\n",
      "16344     412.295644\n",
      "16345     416.498174\n",
      "16346     420.701265\n",
      "16347     424.904919\n",
      "16348     429.109134\n",
      "16349     433.313911\n",
      "16350     801.127428\n",
      "16351     807.498480\n",
      "             ...    \n",
      "275251    813.232789\n",
      "275252    803.548415\n",
      "275253    808.921157\n",
      "275254    816.033003\n",
      "275255    823.145800\n",
      "275256    823.145800\n",
      "275257    828.225651\n",
      "275258    835.410353\n",
      "275259    835.410353\n",
      "275260    835.410353\n",
      "291431    841.660312\n",
      "291432    848.879140\n",
      "291433    856.098933\n",
      "291434    863.319691\n",
      "291435    870.541415\n",
      "291436    876.956745\n",
      "291437    884.208187\n",
      "291438    891.460599\n",
      "291439    897.381822\n",
      "291440    904.387655\n",
      "307611    798.491611\n",
      "307612    806.747737\n",
      "307613    806.747737\n",
      "307614    815.152290\n",
      "307615    782.783906\n",
      "307616    787.191543\n",
      "307617    777.052551\n",
      "307618    782.484156\n",
      "307619    788.999546\n",
      "307620    781.740596\n",
      "Name: fv_cost, Length: 286, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#df[df.Date.str.match(r'^2018-06')]\n",
    "#df[df[\\\"col\\\"].str.contains('this|that')==False]\n",
    "# splitting train and test datasets \n",
    "#training 2010-2017, testing on 2018 data\\\n",
    "train_x = df_1015130[df_1015130['dates'].str.contains('/2018') == False]\n",
    "test_x = df_1015130[df_1015130['dates'].str.contains('/2018') == True]\n",
    "#print(test_x.dates)\n",
    "train_x_cust = train_x['fv_cost']\n",
    "test_x_cust = test_x['fv_cost']\n",
    "\n",
    "print(\"train_data \",train_x_cust)\n",
    "print(\"test_data \", test_x_cust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://drivendata.co/blog/benchmark-cold-start-lstm-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3192.000000\n",
       "mean       91.482684\n",
       "std       170.405114\n",
       "min      -156.842600\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%       113.278646\n",
       "max       813.317904\n",
       "Name: fv_cost, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x['fv_cost'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print (train_x[train_x['fv_cost'].str.contains(2016)])\n",
    "n_input = 2016\n",
    "\n",
    "train_x[(train_x == n_input).any(1)].stack()[lambda x: x != n_input].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3189, 1, 3)\n",
      "(3189,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lag_feature(df, lag=1):\n",
    "    if not type(df) == pd.DataFrame:\n",
    "        df = pd.DataFrame(df, columns=['fv_cost'])\n",
    "    \n",
    "    def rename_lag(ser, j):\n",
    "        ser.name = ser.name + f'_{j}'\n",
    "        return ser\n",
    "        \n",
    "    # add a column lagged by `i` steps\n",
    "    for i in range(1, lag + 1):\n",
    "        df = df.join(df.fv_cost.shift(i).pipe(rename_lag, i))\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#lagged_data=pd.DataFrame(lag_feature(train_x_cust))\n",
    "\n",
    "# X, y format taking the first column (original time series) to be the y\n",
    "#X = lagged_data.drop('fv_cost', axis=1).values\n",
    "#y = lagged_data.fv_cost.values\n",
    "\n",
    "#train_x_cust = lagged_data[:,0]\n",
    "#train_y_cust = lagged_data[:,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_training_data(series_data, lag):\n",
    "    \" Converts a series of data into a lagged, scaled sample.\"\n",
    "    # scale training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    cost_vals = scaler.fit_transform(series_data.values.reshape(-1, 1))\n",
    "    \n",
    "    # convert series to lagged features\n",
    "    cost_lagged = lag_feature(cost_vals, lag=lag)\n",
    "\n",
    "    # X, y format taking the first column (original time series) to be the y\n",
    "    X = cost_lagged.drop('fv_cost', axis=1).values\n",
    "    y = cost_lagged.fv_cost.values\n",
    "    \n",
    "    # keras expects 3 dimensional X\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "train_x_cust,train_y_cust, scalar_train = prepare_training_data(train_x['fv_cost'], 3)\n",
    "test_x_cust,test_y_cust,scalar_test = prepare_training_data(test_x['fv_cost'], 3)\n",
    "print(train_x_cust.shape)\n",
    "print(train_y_cust.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "# lag of 24 to simulate smallest cold start window. Our series\n",
    "# will be converted to a num_timesteps x lag size matrix\n",
    "lag =  3\n",
    "\n",
    "# model parameters\n",
    "num_neurons = 30 #24\n",
    "batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "batch_input_shape=(batch_size, 1, lag)\n",
    "\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "#add convolution layer\n",
    "\n",
    "#Convolution2D (https://keras.io/layers/convolutional/) expects the input to be in the format (samples, rows, cols, channels), \n",
    "#which is \"channels-last\". data is in the format (samples, channels, rows, cols). You should be able to fix\n",
    "#this using the optional keyword data_format = 'channels_first' when declaring the Convolution2D layer else use\n",
    "# input_shape=(3,1).\n",
    "# when strides>1 you cannot have dilation>1\n",
    "#activation_func = ['softmax', 'softplus', 'softsign', 'relu', \n",
    "#                   'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "model.add(Conv1D(filters=num_neurons,batch_size=1, kernel_size=3, strides=3, \n",
    "                 padding=\"same\",activation='sigmoid',dilation_rate=1, \n",
    "                 input_shape=(1, 3),data_format='channels_first'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Flatten()) # as flatten is converting data into 1D but we need 3D for our 3lags data here\n",
    "\n",
    "# add LSTM layer - stateful MUST be true here in \n",
    "# order to learn the patterns within a series\n",
    "model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "\n",
    "# followed by a dense layer with a single output for regression\n",
    "model.add(Dense(1))\n",
    "# we can add dropoutlayer after dense as well again\n",
    "# compile\n",
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "num_training_series = consumption_train.series_id.nunique()\n",
    "num_passes_through_data = 3\n",
    "\n",
    "for i in tqdm(range(num_passes_through_data), \n",
    "              total=num_passes_through_data, \n",
    "              desc='Learning Consumption Trends - Epoch'):\n",
    "    \n",
    "    # reset the LSTM state for training on each series\n",
    "    for ser_id, ser_data in consumption_train.groupby('series_id'):\n",
    "\n",
    "        # prepare the data\n",
    "        X, y, scaler = prepare_training_data(ser_data.consumption, lag)\n",
    "\n",
    "        # fit the model: note that we don't shuffle batches (it would ruin the sequence)\n",
    "        # and that we reset states only after an entire X has been fit, instead of after\n",
    "        # each (size 1) batch, as is the case when stateful=False\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 28s - loss: 0.1065\n",
      "Epoch 2/10\n",
      " - 27s - loss: 0.0413\n",
      "Epoch 3/10\n",
      " - 27s - loss: 0.0319\n",
      "Epoch 4/10\n",
      " - 27s - loss: 0.0244\n",
      "Epoch 5/10\n",
      " - 26s - loss: 0.0189\n",
      "Epoch 6/10\n",
      " - 28s - loss: 0.0171\n",
      "Epoch 7/10\n",
      " - 30s - loss: 0.0163\n",
      "Epoch 8/10\n",
      " - 24s - loss: 0.0155\n",
      "Epoch 9/10\n",
      " - 25s - loss: 0.0155\n",
      "Epoch 10/10\n",
      " - 23s - loss: 0.0155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x105bdc8d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "model.fit(train_x_cust, train_y_cust, epochs=10, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "# plot metrics\n",
    "#pyplot.plot(history.history['mse'])\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.09 MSE (0.30 RMSE)\n",
      "Test Score: 0.12 MSE (0.34 RMSE)\n",
      "unscaled test score 734.440283 MSE\n",
      "actual train mean_squared_error  2237.319642074561\n",
      "actual test rmse  47.300313340130856\n",
      "actual train rmse  10.755224410540166\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Estimate model performance\n",
    "# default batch size is 32 so we need to give batch size explicitly if we want different batch size or \n",
    "# input size should be divisible by batch size for stateful LSTM\n",
    "trainScore = model.evaluate(train_x_cust, train_y_cust, batch_size=1, verbose=2)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(test_x_cust, test_y_cust, batch_size=1, verbose=2)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))\n",
    "\n",
    "print('unscaled test score %f MSE' %(scalar_test.inverse_transform(testScore)))\n",
    "yhat_act_test = scalar_test.inverse_transform(testScore)\n",
    "yhat_act_train = scalar_train.inverse_transform(trainScore)\n",
    "#print (math.sqrt(scaler.inverse_transform(testScore)))\n",
    "print (\"actual train mean_squared_error \",mean_squared_error(test_x.fv_cost[-(len(yhat_act_test)):], yhat_act_test))\n",
    "print (\"actual test rmse \",sqrt(mean_squared_error(test_x.fv_cost[-(len(yhat_act_test)):], yhat_act_test)))\n",
    "print (\"actual train rmse \",sqrt(mean_squared_error(train_x.fv_cost[-(len(yhat_act_train)):],yhat_act_train)))\n",
    "#scaler.inverse_transform(testScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch CV experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Function to create model, required for KerasRegressor\n",
    "def create_model(optimizer='adam',strides_num =3,kernel_size =3,num_neurons=1,\n",
    "                 activation_func ='relu',dropout_rate = 0.2):\n",
    "    lag =  3\n",
    "    #num_neurons = 24\n",
    "    batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "    batch_input_shape=(batch_size, 1, lag)\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=num_neurons,batch_size=1, kernel_size=kernel_size, strides=strides_num, \n",
    "                     padding=\"same\",activation=activation_func,dilation_rate=1, input_shape=(1, 3),\n",
    "                     data_format='channels_first'))\n",
    "    model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "\n",
    "    # followed by a dense layer with a single output for regression\n",
    "    model.add(Dense(1))\n",
    "    # we can add dropoutlayer after dense as well again\n",
    "    model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=2,batch_size = 1,epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#activation_func = ['softmax', 'softplus', 'softsign', 'relu', \n",
    "#                   'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "activation_func = ['relu','tanh', 'linear']\n",
    "nuerons=[1,15,30,45]\n",
    "#param_grid = dict(optimizer=optimizer,activation_func = activation_func)\n",
    "param_grid = dict(activation_func = activation_func,num_neurons=nuerons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(train_x_cust, train_y_cust)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FORECAST BIAS\n",
    "forecast_errors = [expected[i]-predictions[i] for i in range(len(expected))]\n",
    "bias = sum(forecast_errors) * 1.0/len(expected)\n",
    "print('Bias: %f' % bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for training\n",
    "trainPredict = model.predict(train_x_cust,batch_size=1)\n",
    "testPredict = model.predict(test_x_cust,batch_size=1)\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(df_1015130)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[lag:len(trainPredict)+lag, :] = trainPredict\n",
    " \n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(df_1015130)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "#testPredictPlot[len(trainPredict)+(lag*2)+1:len(df_1015130)-1, :] = testPredict\n",
    " \n",
    "# plot baseline and predictions\n",
    "plt.plot(df_1015130['fv_cost'])\n",
    "plt.plot(trainPredictPlot)\n",
    "#plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse scaling for a forecasted value\n",
    "cust_id = \"100019\"\n",
    "#y_inv = invert_scale(scaler, test_x_cust, testPredict)\n",
    "yhat_act = scalar_test.inverse_transform(testPredict)\n",
    "rmse = sqrt(mean_squared_error(test_x.fv_cost[-(len(yhat_act)):], yhat_act))\n",
    "print(cust_id,\" Test RMSE: \",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predictions into csv\n",
    "result_prediction = pd.DataFrame()#, original_fv : test_y_cust, prediction_fv :testPredict})\n",
    "#result_prediction['dates'] = test_x['dates']\n",
    "result_prediction['original_fv'] = test_y_cust\n",
    "result_prediction['prediction_fv'] = testPredict\n",
    "\n",
    "result_prediction.to_csv(\"predictions_for_client_1015193.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_test.inverse_transform(test_y_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions for timesteps ahead\n",
    "# 7,14 ,21 timesteps ahead\n",
    "#we need atleast 4 elements to prepare training data\n",
    "futureElement = testPredict[-4:]\n",
    "\n",
    "def prepare_training_data_array(series_data, lag):\n",
    "    \" Converts a series of data into a lagged, scaled sample.\"\n",
    "    # scale training data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    cost_vals = scaler.fit_transform(series_data.reshape(-1, 1))\n",
    "    \n",
    "    # convert series to lagged features\n",
    "    cost_lagged = lag_feature(cost_vals, lag=lag)\n",
    "\n",
    "    # X, y format taking the first column (original time series) to be the y\n",
    "    X = cost_lagged.drop('fv_cost', axis=1).values\n",
    "    y = cost_lagged.fv_cost.values\n",
    "    \n",
    "    # keras expects 3 dimensional X\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape)\n",
    "x=futureElement[:3]\n",
    "#def timestep_ahead(num,model,futureElement):\n",
    "#futureElement = futureElement_x\n",
    "def timestep_ahead(num,model,futureElement,x):\n",
    "    futureElements = []\n",
    "    futureElements.append(futureElement.tolist())\n",
    "\n",
    "    for i in range(num):\n",
    "        futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "        print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "        print(\"future element value after preparing data\",futureElement_x)\n",
    "        futureElement = model.predict(futureElement_x,batch_size=1,verbose=2)\n",
    "        futureElements.append(futureElement)\n",
    "        print(\"future element before appending x\",futureElement)\n",
    "        futureElement = np.append(futureElement,x)\n",
    "        x=futureElement[:3]\n",
    "        print(\"future element after appending x\",futureElement)\n",
    "        print(\"iteration \",i,\"  complete\")\n",
    "    return (futureElement[0],futureElements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(futureElements[0][0][0][:2])\n",
    "\n",
    "#future_x = np.array(futureElement,futureElements[0][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_7time_ahead,elements = timestep_ahead(7,model,futureElement,x)\n",
    "print (\"scaled prediction value on 7th day ahead \",_7time_ahead)\n",
    "_14time_ahead,elements  = timestep_ahead(14,model,futureElement,x)\n",
    "print (\"scaled prediction value on 14th day ahead \",_14time_ahead)\n",
    "_21time_ahead,elements  = timestep_ahead(21,model,futureElement,x)\n",
    "print (\"scaled prediction value on 21st day ahead \",_21time_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3months ahead predicitons\n",
    "_91days_ahead,elements = timestep_ahead(91,model,futureElement,x)\n",
    "print (\"scaled prediction value on 91st day ahead \",_91days_ahead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pred =[np.array(arr).tolist() for arr in elements]\n",
    "daily_pred\n",
    "def flatten(l): return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]\n",
    "pred_list = flatten(daily_pred)\n",
    "\n",
    "import csv \n",
    "myFile = open('daily_predictions_3months.csv', 'w')  \n",
    "with myFile:  \n",
    "   writer = csv.writer(myFile)\n",
    "   writer.writerow(pred_list)\n",
    "\n",
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find every 7th element into a list\n",
    "week_end_pred = pred_list[0::7]\n",
    "week_end_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly average for 3 months\n",
    "weekly_avg_pred = [(sum(pred_list[x:x+7]))/7 for x in range(0, len(pred_list), 7)]\n",
    "weekly_avg_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find average for every 7 days\n",
    "import itertools\n",
    "n=7 #7days\n",
    "list_week = list(itertools.chain.from_iterable([i]*7 for i in [sum(pred_list[i:i+7])//7 for i in range(0,len(pred_list),7)]))\n",
    "list_week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for i in range(7):\n",
    "futureElement = model.predict(futureElement,batch_size=1)\n",
    "futureElements.append(futureElement)\n",
    "futureElement = np.append(futureElement, futureElements[0][0][0][:3])\n",
    "x=futureElement[:3]\n",
    "print(x)\n",
    "print(\"iteration 0 complete\")\n",
    "#    #return futureElements\n",
    "print(futureElement)\n",
    "print(futureElements)\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "print(\"future element value after preparing data\",futureElement_x)\n",
    "futureElement = model.predict(futureElement_x,batch_size=1)\n",
    "futureElements.append(futureElement)\n",
    "print(\"future element before appending x\",futureElement)\n",
    "futureElement = np.append(futureElement,x)\n",
    "x=futureElement[:3]\n",
    "print(\"future element after appending x\",futureElement)\n",
    "print(\"iteration 1 complete\")\n",
    "print(futureElements)\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "print(\"future element value after preparing data\",futureElement_x)\n",
    "futureElement = model.predict(futureElement_x,batch_size=1)\n",
    "futureElements.append(futureElement)\n",
    "print(\"future element before appending x\",futureElement)\n",
    "futureElement = np.append(futureElement,x)\n",
    "x=futureElement[:3]\n",
    "print(\"future element after appending x\",futureElement)\n",
    "print(\"iteration 2 complete\")\n",
    "print(futureElements)\n",
    "\n",
    "print(type(futureElements))\n",
    "print(type(futureElement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [x for sublist in futureElements for x in sublist]\n",
    "print(flat_list[0][0][1])\n",
    "\n",
    "#above function loops in below format\n",
    "#for sublist in futureElements:\n",
    "#    for x in sublist:\n",
    "#        flat_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price.resample('1W')  \n",
    "#aapl.resample('W').mean()\n",
    "#http://benalexkeen.com/resampling-time-series-data-with-pandas/\n",
    "#resampling based on days and then slicing every 7th day\n",
    "#ts.resample('D').interpolate()[::7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.last('3M') #last 3 months data\n",
    "# to sort dates column before getting last 3months\n",
    "#df_sorted = df.sort_values(by=\"Date\",ascending=True) \\\n",
    "#    .set_index(\"Date\")\n",
    "#    .last(\"3M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/305863/how-to-train-lstm-model-on-multiple-time-series-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/67362/shall-i-use-weekly-or-monthly-data-for-forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weekly predicitons\n",
    "#weekly data\n",
    "week_data = df_1015130.set_index('dates').resample('1W').mean()\n",
    "#week_data['dates'] = pd.to_datetime(week_data['dates'], errors='coerce')\n",
    "\n",
    "\n",
    "week_data = week_data.resample('1W')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#week_data['dates'] = pd.to_datetime(week_data['dates'], unit='D',utc=True)\n",
    "week_data = df_1015130\n",
    "##week_data['New']=week_data.dates.map(week_data.set_index('dates').iloc[1:].resample('D').sum().rolling(7,min_periods =1).visit.mean()).shift()\n",
    "week_data['dates'] = pd.to_datetime(week_data['dates'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data.set_index('dates',inplace=True)\n",
    "week_data_resample = week_data.resample('1W').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data_resample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_index = week_data_resample.index(\"/2018\")\n",
    "#l2 = l[:c_index]\n",
    "week_data_resample['dates'] = week_data_resample.index\n",
    "week_data_resample['dates']=week_data_resample['dates'].astype(str)\n",
    "week_data_resample = week_data_resample.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_week = week_data_resample[week_data_resample['dates'].str.contains('2018') == False]\n",
    "test_week = week_data_resample[week_data_resample['dates'].str.contains('2018') == True]\n",
    "\n",
    "train_x_week,train_y_week, scalar_train = prepare_training_data(train_week['fv_cost'], 3)\n",
    "test_x_week,test_y_week,scalar_test = prepare_training_data(test_week['fv_cost'], 3)\n",
    "print(train_x_week.shape)\n",
    "print(train_y_week.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag =  3\n",
    "num_neurons = 24\n",
    "batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "batch_input_shape=(batch_size, 1, lag)\n",
    "\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=24,batch_size=1, kernel_size=3, strides=3, padding=\"same\",activation='relu',dilation_rate=1, input_shape=(1, 3),data_format='channels_first'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x_week, train_y_week, epochs=10, batch_size=batch_size, verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainScore = model.evaluate(train_x_week, train_y_week, batch_size=1, verbose=2)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(test_x_week, test_y_week, batch_size=1, verbose=2)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))\n",
    "print('unscaled test score %f MSE' %(scalar_test.inverse_transform(testScore)))\n",
    "scalar_train.inverse_transform(trainScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for training\n",
    "trainPredict = model.predict(train_x_week,batch_size=1)\n",
    "testPredict = model.predict(test_x_week,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futureElement = testPredict[-4:]\n",
    "\n",
    "futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "print(futureElement_x.shape)\n",
    "x=futureElement[:3]\n",
    "#def timestep_ahead(num,model,futureElement):\n",
    "#futureElement = futureElement_x\n",
    "def timestep_ahead_week(num,model,futureElement,x):\n",
    "    futureElements = []\n",
    "    futureElements.append(futureElement.tolist())\n",
    "\n",
    "    for i in range(num):\n",
    "        futureElement_x,futureElement_y,scalar = prepare_training_data_array(futureElement, 3)\n",
    "        print(futureElement_x.shape,\"shape after preparing data in 1st iteration\")\n",
    "        print(\"future element value after preparing data\",futureElement_x)\n",
    "        futureElement = model.predict(futureElement_x,batch_size=1,verbose=2)\n",
    "        futureElements.append(futureElement)\n",
    "        print(\"future element before appending x\",futureElement)\n",
    "        futureElement = np.append(futureElement,x)\n",
    "        x=futureElement[:3]\n",
    "        print(\"future element after appending x\",futureElement)\n",
    "        print(\"iteration \",i,\"  complete\")\n",
    "    return (futureElement[0],futureElements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_12_weeks_ahead,all_pred = timestep_ahead_week(12,model,futureElement,x)\n",
    "print (\"scaled prediction value on 3rd month ahead \",_12_weeks_ahead)\n",
    "print (\"all predicitons \",all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of arrays to lists\n",
    "#l = [arr.tolist() for arr in l]\n",
    "#[l.tolist() for l in list1]\n",
    "\n",
    "#weekly_pred = [arr.tolist() for arr in all_pred] #list object has no attribute tolist\n",
    "weekly_pred =[np.array(arr).tolist() for arr in all_pred]\n",
    "weekly_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l): return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = flatten(weekly_pred)\n",
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvwriter.writerow(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "myFile = open('weekly_predictions_3months.csv', 'w')  \n",
    "with myFile:  \n",
    "   writer = csv.writer(myFile)\n",
    "   writer.writerow(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions and analysis for customer with highest rmse 1681085\n",
    "df1 = df[df.fv_cost.notnull()]\n",
    "cust_data = df1[df1['client_debtor_number'] == 1681085]\n",
    "del cust_data['client_debtor_number']\n",
    "\n",
    "\n",
    "lag =  3\n",
    "\n",
    "# model parameters\n",
    "num_neurons = 24\n",
    "batch_size = 1  # this forces the lstm to step through each time-step one at a time\n",
    "batch_input_shape=(batch_size, 1, lag)\n",
    "\n",
    "# instantiate a sequential model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=24,batch_size=1, kernel_size=3, strides=3, padding=\"same\",activation='relu',dilation_rate=1, input_shape=(1, 3),data_format='channels_first'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=3, padding=\"same\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=num_neurons, \n",
    "              batch_input_shape=batch_input_shape, return_sequences=False,# as we only want last hidden output \n",
    "              stateful=True))\n",
    "model.add(Dense(1))\n",
    "# we can add dropoutlayer after dense as well again\n",
    "model.add(Dropout(0.2))\n",
    "# compile\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "\n",
    "\n",
    "train_x = cust_data[cust_data['dates'].str.contains('/2018') == False]\n",
    "test_x = cust_data[cust_data['dates'].str.contains('/2018') == True]\n",
    "print(train_x.head(), len(train_x))\n",
    "print (test_x.head(),len(test_x))\n",
    "\n",
    "train_x_cust,train_y_cust, scalar_train = prepare_training_data(train_x['fv_cost'], 3)\n",
    "test_x_cust,test_y_cust,scalar_test = prepare_training_data(test_x['fv_cost'], 3)  \n",
    "model.fit(train_x_cust, train_y_cust, epochs=10, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "    # input size should be divisible by batch size for stateful LSTM\n",
    "trainScore = model.evaluate(train_x_cust, train_y_cust, batch_size=1, verbose=2)\n",
    "testScore = model.evaluate(test_x_cust, test_y_cust, batch_size=1, verbose=2)\n",
    "train_rmse = trainScore ###\n",
    "test_rmse = testScore ###\n",
    "    # generate predictions for training\n",
    "trainPredict = model.predict(train_x_cust,batch_size=1)\n",
    "testPredict = model.predict(test_x_cust,batch_size=1)\n",
    "yhat_act_test = scalar_test.inverse_transform(testPredict)\n",
    "    # report performance\n",
    "rmse_test = sqrt(mean_squared_error(test_x.fv_cost[-(len(yhat_act_test)):], yhat_act_test))\n",
    "print(cust_id,\" Test RMSE: \",rmse_test)\n",
    "actual_error_scores_test = rmse_test ####\n",
    "yhat_act_train = scalar_train.inverse_transform(trainScore)\n",
    "rmse_train = sqrt(mean_squared_error(train_x.fv_cost[-(len(yhat_act_train)):],yhat_act_train))\n",
    "actual_train_rmse = rmse_train #####\n",
    "    # write predictions into csv\n",
    "result_prediction = pd.DataFrame()#, original_fv : test_y_cust, prediction_fv :testPredict})\n",
    "    #result_prediction['dates'] = test_x['dates']\n",
    "result_prediction['original_fv_scaled'] = test_y_cust\n",
    "result_prediction['prediction_scaled'] = testPredict\n",
    "result_prediction['prediction_actual'] = yhat_act_test\n",
    "result_prediction['fv_actual'] = test_x.fv_cost[-(len(yhat_act_test)):]\n",
    "result_prediction.to_csv(\"predictions_1/predictions_for_client_1681085.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
